{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dokumentacja modułu do scrapowania plików PDF z witryn banków\n",
    "#### Wprowadzenie\n",
    "Kod ten jest przeznaczony do automatycznego pobierania plików PDF z witryn banków na podstawie określonych słów kluczowych i zabezpieczeń przed niechcianymi plikami. Wykorzystuje bibliotekę requests do pobierania zawartości stron internetowych, BeautifulSoup do analizy HTML, wget do pobierania plików, a także moduł pandas do manipulacji danymi w formie tabelarycznej.\n",
    "\n",
    "Zaimplementowany moduł umożliwił znaczne ograniczenie pobieranych plików .pdf do tylko i wyłącznie użytecznych w procesie przetwarzania informacji nt. ofert depozytowych banków. Dobrym przykładem niech będzie strona banku Pekao S.A., na której znajduje się ponad 10000 plików .pdf możliwych do pobrania (w wyniku użycia modułu ograniczono tę liczbę do 3(!)).\n",
    "\n",
    "#### Funkcje i kluczowe zmienne:\n",
    "#### __extract_domain_name_from_url(url)\n",
    "Ta funkcja ekstrahuje nazwę domeny z podanego adresu URL. Umożliwia to utworzenie unikalnej nazwy pliku PDF na podstawie domeny.\n",
    "\n",
    "#### download_pdfs_from_url(keywords, stopwords, url, download_path='pdfs')\n",
    "Główna funkcja kodu, która pobiera pliki PDF z podanej witryny bankowej na podstawie słów kluczowych. Wszystkie pobrane pliki są zapisywane w określonym katalogu.\n",
    "\n",
    "- keywords: Lista słów kluczowych, których obecność jest wymagana w adresie URL pliku PDF.\n",
    "- stopwords: Lista słów, których obecność w adresie URL pliku PDF powoduje jego pominięcie.\n",
    "- url: Adres URL witryny bankowej.\n",
    "- download_path: Ścieżka do katalogu, w którym mają być zapisywane pobrane pliki PDF.\n",
    "#### Przykład użycia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filepath = \"./data/bank_adresses.csv\"\n",
    "df = pd.read_csv(filepath_or_buffer=csv_filepath, delimiter=';')\n",
    "\n",
    "keywords = [\"lokat\", \"oszcz\", \"depoz\", \"oproc\"]\n",
    "stopwords = [\"kred\", \"reg\", \"kart\", \"oswiadcz\", \"inwalid\", \"toip\", \"oplat\", \"prowiz\", \"umow\", \"wnios\",\n",
    "             \"wycof\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\"]\n",
    "\n",
    "for label, row in df.iterrows():\n",
    "    try:\n",
    "        if row[\"url\"]:\n",
    "            download_pdfs_from_url(keywords, stopwords, str(row['url']), download_path='pdfs')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wymagania:\n",
    "- requests: Do pobierania zawartości stron internetowych.\n",
    "- BeautifulSoup: Do analizy HTML i wyszukiwania linków do plików PDF.\n",
    "- wget: Do pobierania plików.\n",
    "- pandas: Do manipulacji danymi w formie tabelarycznej.\n",
    "#### Uwagi:\n",
    "- Upewnij się, że katalog docelowy (pdfs) istnieje przed uruchomieniem kodu, w przeciwnym razie zostanie utworzony.\n",
    "- W przypadku problemów związanych z dostępem do witryn internetowych lub nieprawidłowymi danymi w pliku CSV, kod może zgłaszać wyjątki, które warto monitorować w celu zrozumienia przyczyn.\n",
    "- Staraj się przestrzegać zasad etycznych podczas automatycznego pobierania plików z witryn internetowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wget\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "\n",
    "def __extract_domain_name_from_url(url):\n",
    "    start_index = url.find(\"www.\") + len(\"www.\")\n",
    "    end_index = url.find(\".pl\", start_index)\n",
    "    \n",
    "    if start_index != -1 and end_index != -1:\n",
    "        return url[start_index:end_index]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def download_pdfs_from_url(keywords, stopwords, url, download_path='pdfs'):\n",
    "    # Tworzymy katalog, jeśli nie istnieje\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path)\n",
    "\n",
    "    # Pobieramy zawartość strony\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Szukamy linków do plików PDF\n",
    "    pdf_links = soup.find_all('a', href=lambda href: href and href.lower().endswith('.pdf') and any(keyword.lower() in href.lower() for keyword in keywords) and not any(stopword.lower() in href.lower() for stopword in stopwords))\n",
    "    files_counter = 1\n",
    "    # Pobieramy pliki PDF\n",
    "    for pdf_link in pdf_links:\n",
    "        absolute_url = urljoin(url, pdf_link['href'])\n",
    "        print(absolute_url)\n",
    "        pdf_filename = \"pdfs/\" + __extract_domain_name_from_url(url) + \"_\" + str(files_counter) + \".pdf\"\n",
    "        print(f\"Pobieranie: {pdf_filename}\")\n",
    "        files_counter = files_counter + 1\n",
    "        wget.download(absolute_url, pdf_filename)\n",
    "\n",
    "csv_filepath = \"./data/bank_adresses.csv\"\n",
    "df : pd.DataFrame = pd.read_csv(filepath_or_buffer=csv_filepath, delimiter=';')\n",
    "\n",
    "keywords = [\"lokat\", \"oszcz\", \"depoz\", \"oproc\"]\n",
    "stopwords = [\"kred\", \"reg\", \"kart\", \"oswiadcz\", \"inwalid\", \"toip\", \"oplat\", \"prowiz\", \"umow\", \"wnios\",\n",
    "             \"wycof\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\"]\n",
    "\n",
    "for label, row in df.iterrows():\n",
    "    try:\n",
    "        if row[\"url\"]:\n",
    "            download_pdfs_from_url(keywords, stopwords, str(row['url']), download_path='pdfs')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
