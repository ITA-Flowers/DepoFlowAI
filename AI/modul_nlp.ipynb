{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Rozpoznawania Encji Nazwanych (NER) - Trenowanie i Inferencja\n",
    "#### Wprowadzenie\n",
    "Ten notebook Jupyter dokumentuje proces trenowania modelu do rozpoznawania encji nazwanych (NER) przy użyciu wytrenowanego wcześniej modelu BERT do klasyfikacji tokenów. W kodzie użyta została biblioteka transformers do pracy z modelami BERT, pandas do obsługi danych, oraz scikit-learn do implementacji prostego klasyfikatora bazowego.\n",
    "\n",
    "#### Zależności\n",
    "Upewnij się, że masz zainstalowane poniższe biblioteki przed uruchomieniem kodu:\n",
    "\n",
    "- transformers\n",
    "- torch\n",
    "- pandas\n",
    "- scikit-learn\n",
    "#### Dane\n",
    "Dane treningowe są wczytywane z pliku JSONL zawierającego przykłady tekstu i odpowiadające im adnotacje encji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"id\":33,\"text\":\"Bank Handlowy w Warszawie S.A. z siedzibą w Warszawie, ul. Senatorska 16, 00-923 Warszawa, zarejestrowany w rejestrze przedsiębiorców Krajowego Rejestru Sądowego przez Sąd Rejonowy dla m.st. Warszawy w Warszawie, XII Wydział Gospodarczy Krajowego Rejestru Sądowego, pod nr. KRS 000 000 1538; NIP 526-030-02-91; wysokość kapitału zakładowego wynosi 522 638 400 złotych, kapitał został w pełni opłacony. Citi Handlowy, Citigold, CitiOne, CitiKonto są zastrzeżonymi znakami towarowymi należącymi do podmiotów z grupy Citigroup Inc. 01\\/2023Tabela Oprocentowania Kont Obowiązuje od 30 stycznia 2023 r. Oprocentowanie na Koncie Oszczędnościowym PLN USD GBP 3,00% 0,01% 0,01% Oprocentowanie na Koncie SuperOszczędnościowym PLN USD GBP do 20 000 6,00% 0,05% 0,05% powyżej 20 000 3,00% 0,00% 0,00% Oprocentowanie na Koncie Osobistym PLNUSD, GBP, CHF, EUR, AUD, CAD, ZAR, SEK, NOK, DKK, CZK, HUF 0,00% 0,00%\",\n",
    " \"label\":[[615,638,\"PRODUCT_TYPE\"],[651,656,\"PERCENTAGE\"],[657,662,\"PERCENTAGE\"],[663,668,\"PERCENTAGE\"],[687,693,\"PRODUCT_TYPE\"],[694,715,\"OFFER_NAME\"],[728,737,\"MAX_DEPOSIT\"],[738,743,\"PERCENTAGE\"],[756,770,\"MAX_DEPOSIT\"],[771,776,\"PERCENTAGE\"],[777,782,\"PERCENTAGE\"],[783,788,\"PERCENTAGE\"],[807,813,\"OFFER_TYPE\"]],\"Comments\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicjalizacja Modelu\n",
    "Wczytany zostaje wytrenowany wcześniej model BERT dla języka polskiego wraz z odpowiadającym mu tokenizatorem. Model jest skonfigurowany do klasyfikacji tokenów z określoną liczbą etykiet (encji)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\", num_labels=6)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przygotowanie Danych\n",
    "Funkcja prepare_data jest zdefiniowana do przekształcania surowego tekstu i adnotacji encji na tokeny, etykiety i maski uwagi odpowiednie do trenowania modelu BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(text, annotations, tokenizer, max_length):\n",
    "    # ... (szczegóły implementacji znajdują się w ostatnim akapicie pt. kod źródłowy)\n",
    "    return tokens, labels, attention_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trenowanie Modelu\n",
    "Następnie kod przystępuje do trenowania modelu BERT przy użyciu przygotowanych danych. Używany jest prosty pętla trenująca z optymalizatorem AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "for epoch in range(5):\n",
    "    for _, row in train_data_df.iterrows():\n",
    "        # ... (szczegóły pętli trenującej znajdują się w ostatnim akapicie pt. kod źródłowy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Rozpoznawania Encji Nazwanych (NER)\n",
    "W końcu tworzony jest pipeline NER do dokonywania predykcji na nowych przykładach tekstu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Przykład wykorzystania:\n",
    "\n",
    "tekst_przykładowy = \"Przykładowy tekst do NER.\"\n",
    "encje = nlp_ner(tekst_przykładowy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyuczony model NLP pozawala systemowi wyłuskiwać tylko i wyłączenie słowa kluczowe dotyczące lokat i rachunków oszczędnościowych z plików .pdf zawierających oferty banków dostępnych na ich stronach internetowych. Dzięki współpracy z modelem klasyfikacyjnym jest w stanie z dużą dokładnością wyłuskać treść ofert depozytowych nadającą się do analizy w środowiskach programistycznych.\n",
    "\n",
    "#### Kod źródłowy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "# Ścieżki do danych\n",
    "FULL_TRAINING_SET_FILE = \"D:\\Projekty\\Modules\\scrapper\\data\\dataset.jsonl\"\n",
    "\n",
    "# Wczytanie danych treningowych\n",
    "full_train_df : pd.DataFrame = pd.read_json(FULL_TRAINING_SET_FILE)\n",
    "train_data_df = full_train_df[:20]\n",
    "print(train_data_df.head())\n",
    "\n",
    "# Funkcja przygotowująca dane treningowe\n",
    "def prepare_data(text, annotations, tokenizer, max_length):\n",
    "    encoded = tokenizer.encode_plus(text, return_offsets_mapping=True, max_length=max_length, padding='max_length', truncation=True)\n",
    "    tokens = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    offsets = encoded['offset_mapping']\n",
    "\n",
    "    labels = [-100] * max_length\n",
    "\n",
    "    # Mapowanie adnotacji na poziomie znaków do poziomu tokenów\n",
    "    for start_char, end_char, label in annotations:\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        for token_index, (start, end) in enumerate(offsets):\n",
    "            if (start_token is None) and (start <= start_char) and (end >= start_char):\n",
    "                start_token = token_index\n",
    "            if (end_token is None) and (start <= end_char) and (end >= end_char):\n",
    "                end_token = token_index\n",
    "                break\n",
    "        if start_token is not None and end_token is not None:\n",
    "            labels[start_token:end_token + 1] = [1] * (end_token - start_token + 1)  # Ustawienie etykiet\n",
    "\n",
    "    return tokens, labels, attention_mask\n",
    "\n",
    "# Wczytanie pre-trenowanego modelu BERT dla klasyfikacji tokenów\n",
    "model = BertForTokenClassification.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\", num_labels=6)  # num_labels to liczba klas encji\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")\n",
    "\n",
    "# Proces trenowania modelu\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "for epoch in range(5):\n",
    "    for _, row in train_data_df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        annotations = row[\"label\"]\n",
    "        tokens, labels, attention_mask = prepare_data(text, annotations, tokenizer, max_length=512)\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor([tokens]),\n",
    "            \"attention_mask\": torch.tensor([attention_mask]),\n",
    "            \"labels\": torch.tensor([labels])\n",
    "        }\n",
    "        model.train()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Utworzenie pipeline do rozpoznawania encji\n",
    "nlp_ner = pipeline('ner', model=model, tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
